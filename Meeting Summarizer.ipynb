{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ae98dc-68b6-4119-bea2-9bdf07329c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.speech import SpeechConfig\n",
    "speech_config = SpeechConfig(subscription=\"GEBsBQkxx8vhjmrE9UtSBrP3M6uIRSex6IBs2kGsphOVAn0Epl7WJQQJ99BJACGhslBXJ3w3AAAYACOGhHgY\", region=\"Central India\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6427db6b-4a70-41a2-ad3d-e189b1d3f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_azure_speech(audio_path):\n",
    "    \"\"\"Transcribe using Azure AI Speech-to-Text with continuous recognition\"\"\"\n",
    "    if not speech_config:\n",
    "        raise ValueError(\"Azure Speech client not initialized\")\n",
    "    if not os.path.exists(audio_path):\n",
    "        raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
    "    \n",
    "    print(\"🎤 Transcribing with Azure AI Speech-to-Text...\")\n",
    "    \n",
    "    # Convert MP3 to WAV (Azure prefers WAV for batch processing)\n",
    "    import pydub\n",
    "    temp_wav = \"temp_audio.wav\"\n",
    "    audio = pydub.AudioSegment.from_mp3(audio_path)\n",
    "    audio.export(temp_wav, format=\"wav\")\n",
    "    \n",
    "    # Setup audio configuration\n",
    "    audio_config = AudioConfig(filename=temp_wav)\n",
    "    recognizer = SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "    \n",
    "    # Collect results\n",
    "    transcriptions = []\n",
    "    done = False\n",
    "\n",
    "    def handle_recognized(evt):\n",
    "        if evt.result.text:\n",
    "            transcriptions.append(evt.result.text)\n",
    "    \n",
    "    def handle_done(evt):\n",
    "        nonlocal done\n",
    "        done = True\n",
    "    \n",
    "    recognizer.recognized.connect(handle_recognized)\n",
    "    recognizer.session_stopped.connect(handle_done)\n",
    "    recognizer.canceled.connect(handle_done)\n",
    "    \n",
    "    # Start continuous recognition\n",
    "    recognizer.start_continuous_recognition()\n",
    "    \n",
    "    # Wait until done\n",
    "    while not done:\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    recognizer.stop_continuous_recognition()\n",
    "    \n",
    "    # Clean up temporary WAV\n",
    "    if os.path.exists(temp_wav):\n",
    "        os.remove(temp_wav)\n",
    "    \n",
    "    if not transcriptions:\n",
    "        raise RuntimeError(\"No speech recognized or transcription failed\")\n",
    "    \n",
    "    return \" \".join(transcriptions), \"Azure Speech-to-Text\"\n",
    "\n",
    "def get_transcript(audio_path):\n",
    "    \"\"\"Wrapper for Azure transcription\"\"\"\n",
    "    return transcribe_azure_speech(audio_path)\n",
    "\n",
    "print(\"✅ Transcription functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9dd76f-e614-44b7-86a9-09070538107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def make_summary(transcript):\n",
    "    \"\"\"Generate structured summary with OpenAI or local fallback\"\"\"\n",
    "    prompt = f\"\"\"You are a professional meeting assistant. Summarize this meeting transcript in a structured format:\n",
    "- **Overview**: Date (October 13, 2025), participants (if known, else generic), objectives.\n",
    "- **Key Decisions**: Main decisions as bullets.\n",
    "- **Action Items**: Who does what, by when (if mentioned, else note 'unspecified').\n",
    "- **Next Steps**: Future plans or follow-ups.\n",
    "\n",
    "Be concise, use bullets, and only include what's in the transcript. If unclear, note it.\n",
    "Transcript: {transcript[:8000]}\"\"\"\n",
    "    \n",
    "    if client:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.3\n",
    "            )\n",
    "            return response.choices[0].message.content, \"OpenAI GPT-3.5\"\n",
    "        except Exception as e:\n",
    "            print(f\"❌ OpenAI failed: {e}. Using Hugging Face fallback...\")\n",
    "    \n",
    "    # Fallback to Hugging Face\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    chunk = transcript[:1000]\n",
    "    summary = summarizer(chunk, max_length=150, min_length=50, do_sample=False)\n",
    "    return summary[0][\"summary_text\"], \"Hugging Face BART\"\n",
    "\n",
    "print(\"✅ Summarization function ready!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc34f7ec-af83-40fe-8d37-add1315669fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_results(audio_path, transcript, summary, provider):\n",
    "    \"\"\"Save to JSON and SQLite\"\"\"\n",
    "    result = {\n",
    "        \"filename\": os.path.basename(audio_path),\n",
    "        \"provider\": provider,\n",
    "        \"transcript\": transcript,\n",
    "        \"summary\": summary,\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    json_path = f\"summary_{os.path.basename(audio_path)[:-4]}.json\"\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "    \n",
    "    db = SessionLocal()\n",
    "    meeting = Meeting(filename=os.path.basename(audio_path), transcript=transcript, summary=summary)\n",
    "    db.add(meeting)\n",
    "    db.commit()\n",
    "    db.refresh(meeting)\n",
    "    db.close()\n",
    "    \n",
    "    return meeting.id, json_path\n",
    "\n",
    "print(\"✅ Storage function ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279e3dd7-1b80-4c09-900c-a34c6c82f0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "    print(f\"Contents of directory '{os.path.dirname(audio_file_path)}': {os.listdir(os.path.dirname(audio_file_path))}\")\n",
    "\n",
    "    transcript = \"\"\n",
    "    summary = \"\"\n",
    "    action_items = []\n",
    "    transcript_provider = \"N/A\"\n",
    "    summary_provider = \"N/A\"\n",
    "    action_item_provider = \"N/A\"\n",
    "    meeting_id = \"N/A\"\n",
    "    json_path = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        # Step 1: Transcribe audio\n",
    "        print(\"\\n--- STEP 1: Transcribing Audio ---\")\n",
    "        transcript, transcript_provider = get_transcript(audio_file_path)\n",
    "\n",
    "        if not transcript:\n",
    "            print(\"No speech recognized. Skipping summary and action item generation.\")\n",
    "            summary = \"No speech recognized in audio. Cannot summarize.\"\n",
    "            action_items = [\"No action items due to lack of speech.\"]\n",
    "            summary_provider = \"N/A\"\n",
    "            action_item_provider = \"N/A\"\n",
    "        else:\n",
    "            print(\"\\n--- Transcription Result (first 500 chars) ---\")\n",
    "            print(f\"Provider: {transcript_provider}\")\n",
    "            print(transcript[:500] + (\"...\" if len(transcript) > 500 else \"\"))\n",
    "\n",
    "            # Step 2: Generate Summary\n",
    "            print(\"\\n--- STEP 2: Generating Summary ---\")\n",
    "            summary, summary_provider = make_summary(transcript)\n",
    "            print(f\"Provider: {summary_provider}\")\n",
    "            print(\"Summary:\\n\", summary)\n",
    "\n",
    "            # Step 3: Extract Action Items\n",
    "            print(\"\\n--- STEP 3: Extracting Action Items ---\")\n",
    "            action_items, action_item_provider = extract_action_items(transcript)\n",
    "            print(f\"Provider: {action_item_provider}\")\n",
    "            if action_items:\n",
    "                print(\"Action Items:\")\n",
    "                for item in action_items:\n",
    "                    print(item)\n",
    "            else:\n",
    "                print(\"No action items found.\")\n",
    "\n",
    "        # Step 4: Store Results\n",
    "        print(\"\\n--- STEP 4: Storing Results ---\")\n",
    "        meeting_id, json_path = store_results(\n",
    "            audio_file_path, transcript, summary, action_items,\n",
    "            transcript_provider, summary_provider, action_item_provider\n",
    "        )\n",
    "        print(f\"Saved to {json_path if json_path else 'database (error saving JSON)'} (ID: {meeting_id})\")\n",
    "\n",
    "    except RuntimeError as re:\n",
    "        print(f\"\\n--- An expected runtime error occurred ---\")\n",
    "        print(f\"Error: {re}\")\n",
    "        # Here we capture specific errors like SPXERR_INVALID_HEADER or authorization failures\n",
    "        # and store a partial result indicating the failure.\n",
    "        meeting_id, json_path = store_results(\n",
    "            audio_file_path, transcript, f\"Error during processing: {re}\", [\"N/A\"],\n",
    "            transcript_provider, \"N/A\", \"N/A\"\n",
    "        )\n",
    "        print(f\"Failure logged with ID: {meeting_id}. Partial results saved to: {json_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An unexpected error occurred during processing ---\")\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print full traceback for detailed debugging\n",
    "        # Store a failure record even for unexpected errors\n",
    "        meeting_id, json_path = store_results(\n",
    "            audio_file_path, transcript, f\"Unexpected error: {e}\", [\"N/A\"],\n",
    "            transcript_provider, \"N/A\", \"N/A\"\n",
    "        )\n",
    "        print(f\"Failure logged with ID: {meeting_id}. Partial results saved to: {json_path}\")\n",
    "\n",
    "\n",
    "# --- Main entry point when script is run ---\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file_path = \"C:/Users/Shravani Angalagunta/Downloads/Planning_Meeting.wav\" # Update this if needed\n",
    "\n",
    "    # Call the main orchestrator function\n",
    "    main(audio_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
