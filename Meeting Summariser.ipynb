{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3062678-b070-45ff-9de8-4e812fc8e1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: No API key found! Check your .env file.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    print(\"Error: No API key found! Check your .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b33b981-7e71-4999-af1e-289fe9822e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"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\")\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Test\"}]\n",
    "    )\n",
    "    print(\"Key works!\")\n",
    "except Exception as e:\n",
    "    print(\"Key error:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e122b332-2803-456f-a93f-5cdf4e9b6e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure.cognitiveservices.speech\n",
      "  Downloading azure_cognitiveservices_speech-1.46.0-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting azure-core>=1.31.0 (from azure.cognitiveservices.speech)\n",
      "  Downloading azure_core-1.35.1-py3-none-any.whl.metadata (46 kB)\n",
      "     ---------------------------------------- 0.0/46.5 kB ? eta -:--:--\n",
      "     -------- ------------------------------- 10.2/46.5 kB ? eta -:--:--\n",
      "     ------------------------- ------------ 30.7/46.5 kB 330.3 kB/s eta 0:00:01\n",
      "     -------------------------------------- 46.5/46.5 kB 390.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\shravani angalagunta\\anaconda3\\lib\\site-packages (from azure-core>=1.31.0->azure.cognitiveservices.speech) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\shravani angalagunta\\anaconda3\\lib\\site-packages (from azure-core>=1.31.0->azure.cognitiveservices.speech) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\shravani angalagunta\\anaconda3\\lib\\site-packages (from azure-core>=1.31.0->azure.cognitiveservices.speech) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shravani angalagunta\\anaconda3\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure.cognitiveservices.speech) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shravani angalagunta\\anaconda3\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure.cognitiveservices.speech) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shravani angalagunta\\anaconda3\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure.cognitiveservices.speech) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shravani angalagunta\\anaconda3\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure.cognitiveservices.speech) (2024.2.2)\n",
      "Downloading azure_cognitiveservices_speech-1.46.0-py3-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.4 MB 1.7 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.1/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.2/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.3/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.3/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.4/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.5/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.5/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.6/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.7/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.7/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.8/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.8/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.9/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.9/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 1.0/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.1/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 1.1/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.2/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.3/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.3/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.4/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.4/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.5/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.6/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.6/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.7/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.7/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.8/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.8/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.9/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.0/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.0/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.2/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.2/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.3/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.3/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.4/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 1.3 MB/s eta 0:00:00\n",
      "Downloading azure_core-1.35.1-py3-none-any.whl (211 kB)\n",
      "   ---------------------------------------- 0.0/211.8 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 41.0/211.8 kB 2.0 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 92.2/211.8 kB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 153.6/211.8 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 211.8/211.8 kB 1.3 MB/s eta 0:00:00\n",
      "Installing collected packages: azure-core, azure.cognitiveservices.speech\n",
      "Successfully installed azure-core-1.35.1 azure.cognitiveservices.speech-1.46.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install azure.cognitiveservices.speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8a48d20-0f9a-482c-b950-75617623b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.speech import SpeechConfig\n",
    "speech_config = SpeechConfig(subscription=\"GEBsBQkxx8vhjmrE9UtSBrP3M6uIRSex6IBs2kGsphOVAn0Epl7WJQQJ99BJACGhslBXJ3w3AAAYACOGhHgY\", region=\"Central India\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3118fb0a-8699-457a-a4a3-88626f48d829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Transcription functions ready!\n"
     ]
    }
   ],
   "source": [
    "def transcribe_azure_speech(audio_path):\n",
    "    \"\"\"Transcribe using Azure AI Speech-to-Text with continuous recognition\"\"\"\n",
    "    if not speech_config:\n",
    "        raise ValueError(\"Azure Speech client not initialized\")\n",
    "    if not os.path.exists(audio_path):\n",
    "        raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
    "    \n",
    "    print(\"ðŸŽ¤ Transcribing with Azure AI Speech-to-Text...\")\n",
    "    \n",
    "    # Convert MP3 to WAV (Azure prefers WAV for batch processing)\n",
    "    import pydub\n",
    "    temp_wav = \"temp_audio.wav\"\n",
    "    audio = pydub.AudioSegment.from_mp3(audio_path)\n",
    "    audio.export(temp_wav, format=\"wav\")\n",
    "    \n",
    "    # Setup audio configuration\n",
    "    audio_config = AudioConfig(filename=temp_wav)\n",
    "    recognizer = SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "    \n",
    "    # Collect results\n",
    "    transcriptions = []\n",
    "    done = False\n",
    "\n",
    "    def handle_recognized(evt):\n",
    "        if evt.result.text:\n",
    "            transcriptions.append(evt.result.text)\n",
    "    \n",
    "    def handle_done(evt):\n",
    "        nonlocal done\n",
    "        done = True\n",
    "    \n",
    "    recognizer.recognized.connect(handle_recognized)\n",
    "    recognizer.session_stopped.connect(handle_done)\n",
    "    recognizer.canceled.connect(handle_done)\n",
    "    \n",
    "    # Start continuous recognition\n",
    "    recognizer.start_continuous_recognition()\n",
    "    \n",
    "    # Wait until done\n",
    "    while not done:\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    recognizer.stop_continuous_recognition()\n",
    "    \n",
    "    # Clean up temporary WAV\n",
    "    if os.path.exists(temp_wav):\n",
    "        os.remove(temp_wav)\n",
    "    \n",
    "    if not transcriptions:\n",
    "        raise RuntimeError(\"No speech recognized or transcription failed\")\n",
    "    \n",
    "    return \" \".join(transcriptions), \"Azure Speech-to-Text\"\n",
    "\n",
    "def get_transcript(audio_path):\n",
    "    \"\"\"Wrapper for Azure transcription\"\"\"\n",
    "    return transcribe_azure_speech(audio_path)\n",
    "\n",
    "print(\"âœ… Transcription functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c89e8e1-8556-4352-9c8b-3e0de81afd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Summarization function ready!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def make_summary(transcript):\n",
    "    \"\"\"Generate structured summary with OpenAI or local fallback\"\"\"\n",
    "    prompt = f\"\"\"You are a professional meeting assistant. Summarize this meeting transcript in a structured format:\n",
    "- **Overview**: Date (October 13, 2025), participants (if known, else generic), objectives.\n",
    "- **Key Decisions**: Main decisions as bullets.\n",
    "- **Action Items**: Who does what, by when (if mentioned, else note 'unspecified').\n",
    "- **Next Steps**: Future plans or follow-ups.\n",
    "\n",
    "Be concise, use bullets, and only include what's in the transcript. If unclear, note it.\n",
    "Transcript: {transcript[:8000]}\"\"\"\n",
    "    \n",
    "    if client:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.3\n",
    "            )\n",
    "            return response.choices[0].message.content, \"OpenAI GPT-3.5\"\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ OpenAI failed: {e}. Using Hugging Face fallback...\")\n",
    "    \n",
    "    # Fallback to Hugging Face\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    chunk = transcript[:1000]\n",
    "    summary = summarizer(chunk, max_length=150, min_length=50, do_sample=False)\n",
    "    return summary[0][\"summary_text\"], \"Hugging Face BART\"\n",
    "\n",
    "print(\"âœ… Summarization function ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1e1222a-4e23-41f2-9737-329552fdbde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Storage function ready!\n"
     ]
    }
   ],
   "source": [
    "def store_results(audio_path, transcript, summary, provider):\n",
    "    \"\"\"Save to JSON and SQLite\"\"\"\n",
    "    result = {\n",
    "        \"filename\": os.path.basename(audio_path),\n",
    "        \"provider\": provider,\n",
    "        \"transcript\": transcript,\n",
    "        \"summary\": summary,\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    json_path = f\"summary_{os.path.basename(audio_path)[:-4]}.json\"\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "    \n",
    "    db = SessionLocal()\n",
    "    meeting = Meeting(filename=os.path.basename(audio_path), transcript=transcript, summary=summary)\n",
    "    db.add(meeting)\n",
    "    db.commit()\n",
    "    db.refresh(meeting)\n",
    "    db.close()\n",
    "    \n",
    "    return meeting.id, json_path\n",
    "\n",
    "print(\"âœ… Storage function ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f033fac-a9ea-4370-a2d5-762ced13c799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import time\n",
    "import json\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Optional: For summarization and action item extraction using Hugging Face\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "except ImportError:\n",
    "    pipeline = None\n",
    "    print(\"Warning: 'transformers' library not found. Summarization and action item extraction will use basic fallback.\")\n",
    "\n",
    "# --- Configuration & Setup ---\n",
    "dotenv_path = \"C:/Users/Shravani Angalagunta/Downloads/.env\"\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "AZURE_SPEECH_KEY = os.getenv(\"AZURE_SPEECH_KEY\")\n",
    "AZURE_SPEECH_REGION = os.getenv(\"AZURE_SPEECH_REGION\")\n",
    "\n",
    "if not AZURE_SPEECH_KEY or not AZURE_SPEECH_REGION:\n",
    "    raise ValueError(\"Azure Speech key/region not initialized. Please check your .env file.\")\n",
    "\n",
    "summarizer_llm = None\n",
    "action_item_llm = None\n",
    "openai_client = None\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    try:\n",
    "        # Placeholder: In a real app, you'd initialize an OpenAI client here\n",
    "        # from openai import OpenAI\n",
    "        # openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "        print(\"OpenAI API key detected. OpenAI will be prioritized for summary/action items (placeholder for now).\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not set up OpenAI client: {e}. Falling back to Hugging Face or basic methods.\")\n",
    "        OPENAI_API_KEY = None\n",
    "\n",
    "if not OPENAI_API_KEY and pipeline:\n",
    "    try:\n",
    "        print(\"Initializing Hugging Face summarization pipeline...\")\n",
    "        summarizer_llm = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "        print(\"Hugging Face summarization pipeline initialized.\")\n",
    "\n",
    "        print(\"Initializing Hugging Face zero-shot classification for action item detection...\")\n",
    "        action_item_llm = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "        print(\"Hugging Face zero-shot classification for action items initialized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Hugging Face pipelines: {e}. Summarization and action item extraction will be basic.\")\n",
    "        summarizer_llm = None\n",
    "        action_item_llm = None\n",
    "elif not pipeline:\n",
    "    print(\"Hugging Face 'transformers' not installed. Summarization and action item extraction will be basic.\")\n",
    "\n",
    "# --- Azure Speech Transcription Function ---\n",
    "def transcribe_azure_speech(audio_path: str) -> (str, str):\n",
    "    if not os.path.exists(audio_path):\n",
    "        raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
    "    \n",
    "    print(f\"\\n[Azure ASR] Starting transcription of {os.path.basename(audio_path)}...\")\n",
    "    \n",
    "    speech_config = speechsdk.SpeechConfig(subscription=AZURE_SPEECH_KEY, region=AZURE_SPEECH_REGION)\n",
    "    speech_config.speech_recognition_language = \"en-US\"\n",
    "    \n",
    "    file_ext = os.path.splitext(audio_path)[1].lower()\n",
    "    if file_ext != \".wav\": # Explicitly require WAV for now to avoid format issues\n",
    "        raise ValueError(f\"Unsupported file format. Please use a WAV file (16-bit PCM, 44.1/48 kHz). \"\n",
    "                         f\"Detected: {file_ext}. If this is an MP3, convert it to WAV first.\")\n",
    "    \n",
    "    audio_config = speechsdk.audio.AudioConfig(filename=audio_path)\n",
    "    recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "    \n",
    "    transcript_segments = []\n",
    "    done = False\n",
    "\n",
    "    def stop_cb(evt):\n",
    "        nonlocal done\n",
    "        done = True\n",
    "        print(f\"[Azure ASR] Session stopped event: {evt}\")\n",
    "        recognizer.stop_continuous_recognition_async()\n",
    "\n",
    "    def recognized_cb(evt):\n",
    "        if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "            transcript_segments.append(evt.result.text)\n",
    "            print(f\"[Azure ASR] Recognized: {evt.result.text}\")\n",
    "        elif evt.result.reason == speechsdk.ResultReason.NoMatch:\n",
    "            print(\"[Azure ASR] No speech could be recognized in this segment (NoMatch event).\")\n",
    "        \n",
    "    def canceled_cb(evt):\n",
    "        print(f\"[Azure ASR] Canceled event: {evt.reason} ({evt.error_details})\")\n",
    "        nonlocal done\n",
    "        done = True\n",
    "        recognizer.stop_continuous_recognition_async()\n",
    "        if \"0xa\" in evt.error_details or \"SPXERR_INVALID_HEADER\" in evt.error_details:\n",
    "             raise RuntimeError(\n",
    "                f\"Transcription CANCELED due to audio file issue (SPXERR_INVALID_HEADER or similar). \"\n",
    "                f\"This means the WAV file header might be malformed or the exact format is not what Azure expects. \"\n",
    "                f\"Please ensure it's a standard 16-bit PCM WAV (e.g., 44.1 kHz). Original error: {evt.error_details}\"\n",
    "             )\n",
    "        elif \"AuthorizationFailure\" in evt.error_details:\n",
    "            raise RuntimeError(\n",
    "                f\"Transcription CANCELED due to Authorization Failure. Check your Azure Speech key and region. \"\n",
    "                f\"Original error: {evt.error_details}\"\n",
    "            )\n",
    "\n",
    "    recognizer.recognized.connect(recognized_cb)\n",
    "    recognizer.session_stopped.connect(stop_cb)\n",
    "    recognizer.canceled.connect(canceled_cb)\n",
    "\n",
    "    try:\n",
    "        recognizer.start_continuous_recognition_async().get()\n",
    "        start_time = time.time()\n",
    "        MAX_RECOGNITION_DURATION_SECONDS = 900 # 15 minutes max\n",
    "        while not done and (time.time() - start_time) < MAX_RECOGNITION_DURATION_SECONDS:\n",
    "            time.sleep(1)\n",
    "        if not done:\n",
    "            recognizer.stop_continuous_recognition_async().get()\n",
    "            print(f\"[Azure ASR] Warning: Recognition stopped after {MAX_RECOGNITION_DURATION_SECONDS} seconds due to timeout. \"\n",
    "                  \"Real-time API is best for shorter audios. Consider Azure batch transcription for longer files.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Transcription failed: {str(e)}. Check WAV format and Azure quota.\")\n",
    "    \n",
    "    full_transcript = \" \".join(transcript_segments).strip()\n",
    "    \n",
    "    if not full_transcript:\n",
    "        print(\"[Azure ASR] No transcription produced. This could mean silent audio, low volume, or content not recognized as speech.\")\n",
    "        return \"\", \"Azure Speech-to-Text Real-Time API\"\n",
    "    \n",
    "    print(\"[Azure ASR] Transcription complete.\")\n",
    "    return full_transcript, \"Azure Speech-to-Text Real-Time API\"\n",
    "\n",
    "def get_transcript(audio_path):\n",
    "    return transcribe_azure_speech(audio_path)\n",
    "\n",
    "\n",
    "# --- LLM Functions for Summarization and Action Items (Placeholders/Hugging Face) ---\n",
    "def make_summary(transcript: str) -> (str, str):\n",
    "    if not transcript.strip():\n",
    "        return \"No transcript to summarize.\", \"N/A\"\n",
    "\n",
    "    print(\"\\n[LLM] Generating summary...\")\n",
    "    prompt = \"Summarize the following meeting transcript into key decisions and a concise overview:\"\n",
    "\n",
    "    if OPENAI_API_KEY and openai_client:\n",
    "        print(\"Using a placeholder summary for OpenAI.\")\n",
    "        return f\"Summary (OpenAI placeholder): {transcript[:300]}...\", \"OpenAI Placeholder\"\n",
    "\n",
    "    if summarizer_llm:\n",
    "        try:\n",
    "            max_input_length = summarizer_llm.tokenizer.model_max_length\n",
    "            if len(transcript) > max_input_length * 2:\n",
    "                truncated_transcript = transcript[:max_input_length * 2]\n",
    "                print(f\"[LLM] Warning: Transcript truncated for Hugging Face summarizer. Original length: {len(transcript)} chars.\")\n",
    "            else:\n",
    "                truncated_transcript = transcript\n",
    "\n",
    "            summary_result = summarizer_llm(truncated_transcript, max_length=200, min_length=50, do_sample=False)\n",
    "            summary = summary_result[0]['summary_text'].strip()\n",
    "            return summary, \"Hugging Face DistilBART CNN\"\n",
    "        except Exception as e:\n",
    "            print(f\"[LLM] Error using Hugging Face summarizer: {e}. Providing a basic summary.\")\n",
    "\n",
    "    return f\"Summary (basic fallback): {transcript[:250]}...\", \"Basic Fallback\"\n",
    "\n",
    "def extract_action_items(transcript: str) -> (list[str], str):\n",
    "    if not transcript.strip():\n",
    "        return [], \"N/A\"\n",
    "\n",
    "    print(\"\\n[LLM] Extracting action items...\")\n",
    "    prompt = (\n",
    "        \"From the following meeting transcript, identify and list all clear action items. \"\n",
    "        \"Each action item should be concise and include who is responsible (if mentioned) and what needs to be done. \"\n",
    "        \"List them one per line, preceded by a hyphen. If no action items are present, state 'No action items identified.'\"\n",
    "    )\n",
    "\n",
    "    if OPENAI_API_KEY and openai_client:\n",
    "        print(\"Using a placeholder for OpenAI action item extraction.\")\n",
    "        simulated_items = [\n",
    "            \"- Sarah to draft campaign plan by Oct 20.\",\n",
    "            \"- John to review budget allocations.\",\n",
    "            \"- Team to schedule next meeting.\"\n",
    "        ]\n",
    "        return simulated_items, \"OpenAI Placeholder\"\n",
    "\n",
    "    if action_item_llm:\n",
    "        try:\n",
    "            sentences = [s.strip() for s in transcript.split('.') if s.strip()]\n",
    "            found_actions = []\n",
    "            keywords = [\"need to\", \"should\", \"will be responsible for\", \"assign\", \"task\", \"follow up\", \"due by\", \"plan to\", \"make sure\"]\n",
    "            for sentence in sentences:\n",
    "                if any(k in sentence.lower() for k in keywords):\n",
    "                    candidate_labels = [\"action item\", \"discussion point\", \"information\"]\n",
    "                    classification = action_item_llm(sentence, candidate_labels)\n",
    "                    if classification['labels'][0] == 'action item' and classification['scores'][0] > 0.8:\n",
    "                        found_actions.append(f\"- {sentence}\")\n",
    "            if not found_actions:\n",
    "                return [\"No clear action items identified by Hugging Face (basic keyword detection).\"], \"Hugging Face (Keyword/Zero-Shot)\"\n",
    "            return found_actions, \"Hugging Face (Keyword/Zero-Shot)\"\n",
    "        except Exception as e:\n",
    "            print(f\"[LLM] Error using Hugging Face for action items: {e}. Providing basic extraction.\")\n",
    "\n",
    "    potential_actions = []\n",
    "    for line in transcript.split('.'):\n",
    "        line = line.strip()\n",
    "        if \"need to\" in line.lower() or \"assign\" in line.lower() or \"follow up\" in line.lower():\n",
    "            potential_actions.append(f\"- {line}\")\n",
    "    if not potential_actions:\n",
    "        return [\"No action items identified (basic fallback).\"], \"Basic Fallback\"\n",
    "    return potential_actions, \"Basic Fallback\"\n",
    "\n",
    "\n",
    "# --- Data Storage Function ---\n",
    "def store_results(audio_filepath: str, transcript: str, summary: str, action_items: list[str],\n",
    "                          transcript_provider: str, summary_provider: str, action_item_provider: str) -> (str, str):\n",
    "    output_dir = os.path.dirname(audio_filepath)\n",
    "    filename_stem = os.path.splitext(os.path.basename(audio_filepath))[0]\n",
    "    json_path = os.path.join(output_dir, f\"meeting_summary_{filename_stem}.json\")\n",
    "\n",
    "    meeting_id = str(uuid.uuid4())\n",
    "\n",
    "    results = {\n",
    "        \"meeting_id\": meeting_id,\n",
    "        \"audio_filename\": os.path.basename(audio_filepath),\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"transcript_provider\": transcript_provider,\n",
    "        \"full_transcript\": transcript,\n",
    "        \"summary\": summary,\n",
    "        \"summary_provider\": summary_provider,\n",
    "        \"action_items\": action_items,\n",
    "        \"action_item_provider\": action_item_provider\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"\\n[Storage] Successfully saved results to {json_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Storage] Error saving results to JSON: {e}\")\n",
    "        json_path = None\n",
    "\n",
    "    print(f\"[Storage] Simulating database entry for meeting ID: {meeting_id}\")\n",
    "\n",
    "    return meeting_id, json_path\n",
    "\n",
    "# --- Main Execution Flow Function ---\n",
    "def main(audio_file_path: str):\n",
    "    \"\"\"\n",
    "    Main function to orchestrate transcription, summarization, action item extraction, and storage.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Meeting Summarizer ---\")\n",
    "    print(\"Audio file to process: \" + audio_file_path)\n",
    "\n",
    "    if not os.path.exists(audio_file_path):\n",
    "        print(f\"Error: Audio file not found at {audio_file_path}\")\n",
    "        print(f\"Please ensure the file exists or update the 'audio_file_path' variable.\")\n",
    "        return\n",
    "\n",
    "    size_mb = os.path.getsize(audio_file_path) / (1024 * 1024)\n",
    "    print(f\"File: {os.path.basename(audio_file_path)} ({size_mb:.2f} MiB)\")\n",
    "    print(f\"Contents of directory '{os.path.dirname(audio_file_path)}': {os.listdir(os.path.dirname(audio_file_path))}\")\n",
    "\n",
    "    transcript = \"\"\n",
    "    summary = \"\"\n",
    "    action_items = []\n",
    "    transcript_provider = \"N/A\"\n",
    "    summary_provider = \"N/A\"\n",
    "    action_item_provider = \"N/A\"\n",
    "    meeting_id = \"N/A\"\n",
    "    json_path = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        # Step 1: Transcribe audio\n",
    "        print(\"\\n--- STEP 1: Transcribing Audio ---\")\n",
    "        transcript, transcript_provider = get_transcript(audio_file_path)\n",
    "\n",
    "        if not transcript:\n",
    "            print(\"No speech recognized. Skipping summary and action item generation.\")\n",
    "            summary = \"No speech recognized in audio. Cannot summarize.\"\n",
    "            action_items = [\"No action items due to lack of speech.\"]\n",
    "            summary_provider = \"N/A\"\n",
    "            action_item_provider = \"N/A\"\n",
    "        else:\n",
    "            print(\"\\n--- Transcription Result (first 500 chars) ---\")\n",
    "            print(f\"Provider: {transcript_provider}\")\n",
    "            print(transcript[:500] + (\"...\" if len(transcript) > 500 else \"\"))\n",
    "\n",
    "            # Step 2: Generate Summary\n",
    "            print(\"\\n--- STEP 2: Generating Summary ---\")\n",
    "            summary, summary_provider = make_summary(transcript)\n",
    "            print(f\"Provider: {summary_provider}\")\n",
    "            print(\"Summary:\\n\", summary)\n",
    "\n",
    "            # Step 3: Extract Action Items\n",
    "            print(\"\\n--- STEP 3: Extracting Action Items ---\")\n",
    "            action_items, action_item_provider = extract_action_items(transcript)\n",
    "            print(f\"Provider: {action_item_provider}\")\n",
    "            if action_items:\n",
    "                print(\"Action Items:\")\n",
    "                for item in action_items:\n",
    "                    print(item)\n",
    "            else:\n",
    "                print(\"No action items found.\")\n",
    "\n",
    "        # Step 4: Store Results\n",
    "        print(\"\\n--- STEP 4: Storing Results ---\")\n",
    "        meeting_id, json_path = store_results(\n",
    "            audio_file_path, transcript, summary, action_items,\n",
    "            transcript_provider, summary_provider, action_item_provider\n",
    "        )\n",
    "        print(f\"Saved to {json_path if json_path else 'database (error saving JSON)'} (ID: {meeting_id})\")\n",
    "\n",
    "    except RuntimeError as re:\n",
    "        print(f\"\\n--- An expected runtime error occurred ---\")\n",
    "        print(f\"Error: {re}\")\n",
    "        # Here we capture specific errors like SPXERR_INVALID_HEADER or authorization failures\n",
    "        # and store a partial result indicating the failure.\n",
    "        meeting_id, json_path = store_results(\n",
    "            audio_file_path, transcript, f\"Error during processing: {re}\", [\"N/A\"],\n",
    "            transcript_provider, \"N/A\", \"N/A\"\n",
    "        )\n",
    "        print(f\"Failure logged with ID: {meeting_id}. Partial results saved to: {json_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An unexpected error occurred during processing ---\")\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print full traceback for detailed debugging\n",
    "        # Store a failure record even for unexpected errors\n",
    "        meeting_id, json_path = store_results(\n",
    "            audio_file_path, transcript, f\"Unexpected error: {e}\", [\"N/A\"],\n",
    "            transcript_provider, \"N/A\", \"N/A\"\n",
    "        )\n",
    "        print(f\"Failure logged with ID: {meeting_id}. Partial results saved to: {json_path}\")\n",
    "\n",
    "\n",
    "# --- Main entry point when script is run ---\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file_path = \"C:/Users/Shravani Angalagunta/Downloads/Planning_Meeting.wav\" # Update this if needed\n",
    "\n",
    "    # Call the main orchestrator function\n",
    "    main(audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cb70a9-02ba-4eea-8f9b-c0e109458520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
